{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud beta dataproc clusters create cluster-ce8a --enable-component-gateway --bucket its_a_bukket --region us-central1 --subnet default --zone us-central1-b --master-machine-type n1-standard-2 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-2 --worker-boot-disk-size 500 --image-version 1.3-deb9 --optional-components ANACONDA,JUPYTER --max-idle 7200s --scopes 'https://www.googleapis.com/auth/cloud-platform' --project sapient-origin-267302"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful links:\n",
    "\n",
    "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3923635548890252/1357850364289680/4930913221861820/latest.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-features.html#tf-idf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().appName('reddit_pipeline').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0-cdh6.3.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now load the data as a PySpark DataFrame, and confirm the load was successful by reviewing the first five rows of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.28 ms, sys: 55 Âµs, total: 2.34 ms\n",
      "Wall time: 375 ms\n"
     ]
    }
   ],
   "source": [
    "%time df_raw = spark.read.option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").csv('politics_all_politics_all000000000000',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the load worked as intended, I will count missing values by column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+--------+-------+-------+-----------------+-------+-----------+------------+-------+---------+-------+------------+----------------+-------+-------+---------+-------+-------------+----------------------+\n",
      "|body|score_hidden|archived|   name| author|author_flair_text|  downs|created_utc|subreddit_id|link_id|parent_id|  score|retrieved_on|controversiality| gilded|     id|subreddit|    ups|distinguished|author_flair_css_class|\n",
      "+----+------------+--------+-------+-------+-----------------+-------+-----------+------------+-------+---------+-------+------------+----------------+-------+-------+---------+-------+-------------+----------------------+\n",
      "| 115|     1969152| 2057434|2414161|1142312|          2255571|2480318|    1197470|     1094818|1064760|  1052482|1046843|     1044667|         1043137|1042456|1042118|  1041933|2143207|      2505445|               2395523|\n",
      "+----+------------+--------+-------+-------+-----------------+-------+-----------+------------+-------+---------+-------+------------+----------------+-------+-------+---------+-------+-------------+----------------------+\n",
      "\n",
      "CPU times: user 32.7 ms, sys: 18.6 ms, total: 51.4 ms\n",
      "Wall time: 5.78 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "%time df_raw.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_raw.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seee that there are a few rows with missing \"body\" information, so these will have to be dropped from the DataFrame before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+--------+-------+-------+-----------------+-------+-----------+------------+-------+---------+-------+------------+----------------+-------+-------+---------+-------+-------------+----------------------+\n",
      "|body|score_hidden|archived|   name| author|author_flair_text|  downs|created_utc|subreddit_id|link_id|parent_id|  score|retrieved_on|controversiality| gilded|     id|subreddit|    ups|distinguished|author_flair_css_class|\n",
      "+----+------------+--------+-------+-------+-----------------+-------+-----------+------------+-------+---------+-------+------------+----------------+-------+-------+---------+-------+-------------+----------------------+\n",
      "|   0|     1969076| 2057345|2414053|1142273|          2255500|2480205|    1197432|     1094788|1064729|  1052452|1046813|     1044637|         1043107|1042426|1042088|  1041902|2143116|      2505333|               2395436|\n",
      "+----+------------+--------+-------+-------+-----------------+-------+-----------+------------+-------+---------+-------+------------+----------------+-------+-------+---------+-------+-------------+----------------------+\n",
      "\n",
      "CPU times: user 33.4 ms, sys: 14.4 ms, total: 47.8 ms\n",
      "Wall time: 4.52 s\n"
     ]
    }
   ],
   "source": [
    "#select only rows with non-null 'body' values\n",
    "df_raw = df_raw.filter(df_raw.body.isNotNull())\n",
    "\n",
    "#confirm process completed successfully\n",
    "%time df_raw.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_raw.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this initial data cleaning step completed, I will review a sample of the comments to observe their format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(body='almost a third of registered voters in CA are republican, though...'),\n",
       " Row(body=\"So, you're saying netanyahu wants peace?\"),\n",
       " Row(body='But the Saudis said that the refugees are too dangerous to take in and have taken in 0...'),\n",
       " Row(body='Yep, \"opt-out\" has been changed to \"you are permitted to ask\"'),\n",
       " Row(body=\"They can answer however they'd like\")]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.select(\"body\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process this text for machine learning analysis, we will have to remove punctuation as a first step.  This will be accomplished via a regex function that removes any character that is not a letter, number, or space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.27 ms, sys: 2.4 ms, total: 3.67 ms\n",
      "Wall time: 184 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(body_vec='almost a third of registered voters in CA are republican though'),\n",
       " Row(body_vec='So youre saying netanyahu wants peace'),\n",
       " Row(body_vec='But the Saudis said that the refugees are too dangerous to take in and have taken in 0'),\n",
       " Row(body_vec='Yep optout has been changed to you are permitted to ask'),\n",
       " Row(body_vec='They can answer however theyd like')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "#remove formatting characters\n",
    "df_raw = df_raw.withColumn('body_vec', f.regexp_replace('body', \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
    "\n",
    "#confirm output\n",
    "%time df_raw.select(\"body_vec\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will put all letters into lower-case so our hashing algorithm doesn't perceive \"Clinton\" and \"clinton\" as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.1 ms, sys: 1.03 ms, total: 4.12 ms\n",
      "Wall time: 197 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(body_vec='almost a third of registered voters in ca are republican though'),\n",
       " Row(body_vec='so youre saying netanyahu wants peace'),\n",
       " Row(body_vec='but the saudis said that the refugees are too dangerous to take in and have taken in 0'),\n",
       " Row(body_vec='yep optout has been changed to you are permitted to ask'),\n",
       " Row(body_vec='they can answer however theyd like')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower\n",
    "\n",
    "df_raw = df_raw.withColumn('body_vec', lower(col('body_vec')))\n",
    "\n",
    "%time df_raw.select(\"body_vec\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we establish a custom Pipeline function to stem all tokens in a column using the NLTK Porter Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "#import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#this code courtesy of http://michael-harmon.com/blog/SentimentAnalysisP2.html\n",
    "\n",
    "class PorterStemming(Transformer, HasInputCol, HasOutputCol):\n",
    "    \"\"\"\n",
    "    PosterStemming class using the NLTK Porter Stemmer\n",
    "    \n",
    "    This comes from https://stackoverflow.com/questions/32331848/create-a-custom-transformer-in-pyspark-ml\n",
    "    Adapted to work with the Porter Stemmer from NLTK.\n",
    "    \"\"\"\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, \n",
    "                 inputCol  : str = None, \n",
    "                 outputCol : str = None, \n",
    "                 min_size  : int = None):\n",
    "        \"\"\"\n",
    "        Constructor takes in the input column name, output column name,\n",
    "        plus the minimum legnth of a token (min_size)\n",
    "        \"\"\"\n",
    "        # call Transformer classes constructor since were extending it.\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # set Parameter objects minimum token size\n",
    "        self.min_size = Param(self, \"min_size\", \"\")\n",
    "        self._setDefault(min_size=0)\n",
    "\n",
    "        # set the input keywork arguments\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "        # initialize Stemmer object\n",
    "        self.stemmer  = PorterStemmer()\n",
    "\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, \n",
    "                  inputCol  : str = None, \n",
    "                  outputCol : str = None, \n",
    "                  min_size  : int = None\n",
    "      ) -> None:\n",
    "        \"\"\"\n",
    "        Function to set the keyword arguemnts\n",
    "        \"\"\"\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "\n",
    "    def _stem_func(self, words  : list) -> list:\n",
    "        \"\"\"\n",
    "        Stemmer function call that performs stemming on a\n",
    "        list of tokens in words and returns a list of tokens\n",
    "        that have meet the minimum length requiremnt.\n",
    "        \"\"\"\n",
    "        # We need a way to get min_size and cannot access it \n",
    "        # with self.min_size\n",
    "        min_size       = self.getMinSize()\n",
    "\n",
    "        # stem that actual tokens by applying \n",
    "        # self.stemmer.stem function to each token in \n",
    "        # the words list\n",
    "        stemmed_words  = map(self.stemmer.stem, words)\n",
    "\n",
    "        # now create the new list of tokens from\n",
    "        # stemmed_words by filtering out those\n",
    "        # that are not of legnth > min_size\n",
    "        filtered_words = filter(lambda x: len(x) > min_size, stemmed_words)\n",
    "\n",
    "        return list(filtered_words)\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transform function is the method that is called in the \n",
    "        MLPipleline.  We have to override this function for our own use\n",
    "        and have it call the _stem_func.\n",
    "\n",
    "        Notice how it takes in a type DataFrame and returns type Dataframe\n",
    "        \"\"\"\n",
    "        # Get the names of the input and output columns to use\n",
    "        out_col       = self.getOutputCol()\n",
    "        in_col        = self.getInputCol()\n",
    "\n",
    "        # create the stemming function UDF by wrapping the stemmer \n",
    "        # method function\n",
    "        stem_func_udf = F.udf(self._stem_func, ArrayType(StringType()))\n",
    "        \n",
    "        # now apply that UDF to the column in the dataframe to return\n",
    "        # a new column that has the same list of words after being stemmed\n",
    "        df2           = df.withColumn(out_col, stem_func_udf(df[in_col]))\n",
    "\n",
    "        return df2\n",
    "  \n",
    "  \n",
    "    def setMinSize(self,value):\n",
    "        \"\"\"\n",
    "        This method sets the minimum size value\n",
    "        for the _paramMap dictionary.\n",
    "        \"\"\"\n",
    "        self._paramMap[self.min_size] = value\n",
    "        return self\n",
    "\n",
    "    def getMinSize(self) -> int:\n",
    "        \"\"\"\n",
    "        This method uses the parent classes (Transformer)\n",
    "        .getOrDefault method to get the minimum\n",
    "        size of a token.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.min_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this custom function in place, I will build out a full pipeline that completes the following steps in order:\n",
    "\n",
    "* Tokenize each comment string\n",
    "* Remove stop words\n",
    "* Stem each token\n",
    "* Hashes the tokens\n",
    "* Computes the IDF of those tokens\n",
    "\n",
    "The pipeline will ultimately output a column named \"features\" that can be used for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover, IDF\n",
    "\n",
    "# #tokenize body text\n",
    "# tokenizer = Tokenizer(inputCol=\"body_vec\", outputCol=\"body_vec_token\")\n",
    "# df_raw = tokenizer.transform(df_raw)\n",
    "# df_raw.select('body_vec_token').take(5)\n",
    "\n",
    "# #clean old column\n",
    "# %time df_raw = df_raw.drop('body_vec')\n",
    "\n",
    "# #remove stop words\n",
    "# remover = StopWordsRemover(inputCol = \"body_vec_token\", outputCol = \"body_vec_token_nosw\")\n",
    "# df_raw = remover.transform(df_raw)\n",
    "# df_raw.select('body_vec_token_nosw').take(5)\n",
    "\n",
    "# #clean old column\n",
    "# %time df_raw = df_raw.drop('body_vec_token')\n",
    "\n",
    "# #stem all tokens\n",
    "# stemmer = PorterStemming(inputCol = \"body_vec_token_nosw\", outputCol = \"body_vec_cleaned\")\n",
    "# df_raw = stemmer.transform(df_raw)\n",
    "# df_raw.select('body_vec_cleaned').take(5)\n",
    "\n",
    "# #clean old column\n",
    "# %time df_raw = df_raw.drop('body_vec_token_nosw')\n",
    "\n",
    "# #hashingTF vectorization\n",
    "# hashingTF = HashingTF(inputCol=\"body_vec_cleaned\", outputCol=\"body_vec_tf\")\n",
    "# df_raw = hashingTF.transform(df_raw)\n",
    "# df_raw.select('body_vec_tf').take(5)\n",
    "\n",
    "# #clean old column\n",
    "# %time df_raw = df_raw.drop('body_vec_cleaned')\n",
    "\n",
    "# #IDF vectorization\n",
    "# idf = IDF(inputCol=\"body_vec_tf\", outputCol=\"body_vec_tfidf\")\n",
    "# df_raw = idf.fit(df_raw).transform(df_raw)\n",
    "# df_raw.select('body_vec_tfidf').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"body_vec\", outputCol=\"body_vec_token\")\n",
    "remover = StopWordsRemover(inputCol = \"body_vec_token\", outputCol = \"body_vec_token_nosw\")\n",
    "stemmer = PorterStemming(inputCol = \"body_vec_token_nosw\", outputCol = \"body_vec_cleaned\")\n",
    "hashingTF = HashingTF(inputCol=\"body_vec_cleaned\", outputCol=\"body_vec_tf\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, stemmer, hashingTF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.3 ms, sys: 8.26 ms, total: 51.6 ms\n",
      "Wall time: 282 ms\n"
     ]
    }
   ],
   "source": [
    "%time df_raw = pipeline.fit(df_raw).transform(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.12 ms, sys: 4.87 ms, total: 12 ms\n",
      "Wall time: 7.43 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(body_vec_tf=SparseVector(262144, {7761: 1.0, 12074: 1.0, 68641: 1.0, 74383: 1.0, 145449: 1.0, 197339: 1.0, 223821: 1.0})),\n",
       " Row(body_vec_tf=SparseVector(262144, {55039: 1.0, 65212: 1.0, 180893: 1.0, 190256: 1.0, 204137: 1.0})),\n",
       " Row(body_vec_tf=SparseVector(262144, {8227: 1.0, 55639: 1.0, 75173: 1.0, 82321: 1.0, 82657: 1.0, 168976: 1.0, 235413: 1.0})),\n",
       " Row(body_vec_tf=SparseVector(262144, {1007: 1.0, 109810: 1.0, 172796: 1.0, 221124: 1.0, 248891: 1.0})),\n",
       " Row(body_vec_tf=SparseVector(262144, {86850: 1.0, 181489: 1.0, 198829: 1.0, 208258: 1.0}))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df_raw.select(\"body_vec_tf\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw.drop('body_vec')\n",
    "df_raw = df_raw.drop('body_vec_token')\n",
    "df_raw = df_raw.drop('body_vec_token_nosw')\n",
    "df_raw = df_raw.drop('body_vec_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(body_vec_tfidf=SparseVector(262144, {7761: 4.7521, 12074: 3.7989, 68641: 6.758, 74383: 6.2482, 145449: 7.753, 197339: 4.5417, 223821: 5.294})),\n",
       " Row(body_vec_tfidf=SparseVector(262144, {55039: 3.8732, 65212: 3.241, 180893: 9.2539, 190256: 3.4849, 204137: 6.6828})),\n",
       " Row(body_vec_tfidf=SparseVector(262144, {8227: 7.181, 55639: 3.9531, 75173: 7.287, 82321: 6.0177, 82657: 6.2106, 168976: 3.9227, 235413: 6.9651})),\n",
       " Row(body_vec_tfidf=SparseVector(262144, {1007: 10.7641, 109810: 4.7971, 172796: 6.4791, 221124: 4.506, 248891: 7.9858})),\n",
       " Row(body_vec_tfidf=SparseVector(262144, {86850: 5.0907, 181489: 5.3979, 198829: 6.5195, 208258: 2.7532}))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"body_vec_tf\", outputCol=\"body_vec_tfidf\")\n",
    "\n",
    "df_raw = idf.fit(df_raw).transform(df_raw)\n",
    "\n",
    "df_raw.select('body_vec_tfidf').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(body='almost a third of registered voters in CA are republican, though...', score_hidden=None, archived=None, name=None, author='Starkravingmad7', author_flair_text=None, downs=None, created_utc='1502391062', subreddit_id='t5_2cneq', link_id='t3_6stdi4', parent_id='t1_dlfk7kb', score='2', retrieved_on='1503941308', controversiality='0', gilded='0', id='dlft5kl', subreddit='politics', ups=None, distinguished=None, author_flair_css_class=None, body_vec_tf=SparseVector(262144, {7761: 1.0, 12074: 1.0, 68641: 1.0, 74383: 1.0, 145449: 1.0, 197339: 1.0, 223821: 1.0}), body_vec_tfidf=SparseVector(262144, {7761: 4.7521, 12074: 3.7989, 68641: 6.758, 74383: 6.2482, 145449: 7.753, 197339: 4.5417, 223821: 5.294}))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark 4G 32e",
   "language": "python",
   "name": "pyspark2_4g32e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
